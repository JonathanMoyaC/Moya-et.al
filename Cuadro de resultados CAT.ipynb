{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=center><font size = 5>Machine Learning models for aiding the decision-making process in emergency departments</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Tabla comparativa de algoritmos<h1>\n",
    "<h3>Descripción</h3>\n",
    "<p>\n",
    "En este script se desarrollará una serie de predicciones basadas en datos del hospital San Juan de Dios Curicó, correspondientes al año 2018 representados por registros de urgencias. El objetivo es predecir, mediante un algoritmo de arboles de desición, la categoría de gravedad de un apciente de urgencias,tomando como input, datos proporcionados por el paciente en la etapa de registro y sus signos vitales registrados en la etapa de triage.\n",
    "Se correrán algoritmos de prodicción tales como árboles y bosques de desición, regresión logística, support vector machines y redes neuronales. Para finalmente evaluar el rendimiento de cada algoritmo en términos de la predicción, mediante indicadores tales como Acuraccy, F1-Score, Curva ROC, Índice de Jaccard y logloss\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"Descripción de datos\">Descripción de datos</h1>\n",
    "<p>\n",
    "Los datos utilizados fueros proporcionados por el Hospital San Juan de Dios, Curicó, Chile y corresponden a 4.971 registros de pacientes que asistieron a urgencias durante el periodo comprendido entre el 1 de enero de 2018 y agosto de 2019, los datos fueron limpiados y transformados en un script desarrollado previamente\n",
    "<ul>\n",
    "    <li>Datos: <a href=\"https://drive.google.com/open?id=1Bp7_MnK6cGwgBuwIq1a8S4DS_0wVDiAD\" target=\"_blank\">https://drive.google.com/open?id=1Bp7_MnK6cGwgBuwIq1a8S4DS_0wVDiAD</a></li>\n",
    "    <li>Tipo de datos: csv</li>\n",
    "</ul>\n",
    "<p>\n",
    "Las variables presentes en los datos se describen a continuación:\n",
    "<ul>    \n",
    "   \n",
    "   <li><b>PAC_EDAD</b>: corresponde a la edad del paciente en enteros</li>\n",
    "   <li><b>MOTIVO_CONSULTA</b>: corresponde a la razón por la que el paciente acude al servicio de urgencias string</li>\n",
    "   <li><b>MEDIO</b>: corresponde al medio de llegada, mediante el que el paciente acude al servicio de urgencias</li>\n",
    "   <li><b>SEXO</b>: corresponde al sexo del paciente</li>\n",
    "   <li><b>CAT</b>: corresponde a la categoría de gravedad asignada al paciente en el proceso de Triage</li>\n",
    "   <li><b>PRESION_SIST</b>: corresponde la presión sistólica del paciente </li>\n",
    "   <li><b>PRESION_DIAST</b>: corresponde la presióndiastólica del paciente</li>\n",
    "   <li><b>SATO2</b>: Dato numérico que representa la saturometria del paciente (Nivel de oxigeno en la sangre)</li>\n",
    "   <li><b>TEMPERATURA</b>: corresponde a la temperatura corporal del paciente en el momento de la categorización</li>\n",
    "   <li><b>GLASGOW</b>: corresponde a al nivel registrado por el paciente en la escala Glasgow</li>\n",
    "   <li><b>DM</b>: corresponde si el paciente presenta o no Diabetes Mellitus</li>\n",
    "   <li><b>EVA</b>: corresponde si se aplica al paciente una evaluación de vias aéreas</li>\n",
    "   <li><b>HGT</b>: corresponde a la medida de azucar en la sangre del paciente</li>\n",
    "   <li><b>LCFA</b>: corresponde a si el paciente presenta obstrucción crónica de vías aéreas</li>\n",
    "   <li><b>FR</b>: corresponde a la frecuencia respiratoria del paciente</li>\n",
    "   <li><b>HTA</b>: corresponde a si el paciente posee Hipertención Arterial</li>\n",
    "   <li><b>HORA_INSC</b>: corresponde a la hora en la que el paciente fue categorizado</li>\n",
    "   <li><b>MIN_INSC</b>: corresponde al minuto en que el paciente fue categorizado</li>\n",
    "   <li><b>TIEMPO_ESPERA_CAT</b>: corresponde al tiempo que espera el paciente luego de ser registrado, para ser categorizado</li>\n",
    "      \n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar paquetes necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "import sklearn as sk  \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "#métricas de evaluación\n",
    "from sklearn.metrics import f1_score\n",
    "#from sklearn.metrics import jaccard_similarity_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "#Métodos de tuneo de parámetros\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Métodos para balancer las clases\n",
    "from pylab import rcParams\n",
    " \n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    " \n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lectura de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo= 'df_definitiva_A.csv'\n",
    "archivo2='df_cat_soloTexto.csv'\n",
    "df_cat= pd.read_csv(archivo,encoding='latin-1',sep=\",\")\n",
    "df_PCA= pd.read_csv(archivo2,encoding='latin-1',sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisar si hay columnas sin nombre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat=df_cat.loc[:, ~df_cat.columns.str.contains('^Unnamed')]\n",
    "df_PCA=df_PCA.loc[:, ~df_PCA.columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dejar solo en una base de datos la variable target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PCA.drop(columns =[\"CAT\"], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionar las variables que se utilizarán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat=df_cat[['ID_PACIENTE', 'PAC_EDAD', 'CAT', 'SATO2', 'TEMPERATURA', 'GLASGOW',\n",
    "       'EVA', 'HGT', 'FR','SEXO_M', 'SEXO_F', 'DM_D', 'DM_N',\n",
    "       'DM_S', 'LCFA_D', 'LCFA_N', 'LCFA_S', 'LCFA_D.1', 'LCFA_N.1',\n",
    "       'LCFA_S.1']]\n",
    "df_cat.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unir bases de datos horizontalmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat =  df_cat.join(df_PCA, lsuffix='_caller', rsuffix='_other')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quitar variables innecesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quitando DESC_EVENTO\n",
    "df_cat.drop(columns =[\"DESC_EVENTO\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiando de faltantes las columnas\n",
    "df_cat.dropna(subset=[\"ID_PACIENTE_caller\"], axis=0, inplace = True)\n",
    "df_cat.dropna(subset=[\"ID_PACIENTE_other\"], axis=0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quitando ID_PACIENTE\n",
    "df_cat.drop(columns =[\"ID_PACIENTE_caller\"], inplace = True)\n",
    "df_cat.drop(columns =[\"ID_PACIENTE_other\"], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identificar como \"y\" a la variable target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_cat[\"CAT\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quitar la variable target de la base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quitando CAT\n",
    "df_cat.drop(columns =[\"CAT\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asignar a \"x\" el resto de variables independientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h1 id=\"Preprocesamiento de datos\">Preprocesamiento de datos</h1>\n",
    "<p>\n",
    "Para aplicar el paquete de arboles de desición, los datos deben ser numéricos, en este caso siguen siendo en su mayoría categoricos pero serán transformados a nominales.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocesando motivo de consulta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_motivo_consulta = preprocessing.LabelEncoder()\n",
    "le_motivo_consulta.fit(X[\"MOTIVO_CONSULTA\"])\n",
    "\n",
    "X[\"MOTIVO_CONSULTA\"]=le_motivo_consulta.transform(X[\"MOTIVO_CONSULTA\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocesando medio de llegada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_medio = preprocessing.LabelEncoder()\n",
    "le_medio.fit(X[\"MEDIO\"])\n",
    "X[\"MEDIO\"] =le_medio.transform(X[\"MEDIO\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocesamiento Sexo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_sexo = preprocessing.LabelEncoder()\n",
    "le_sexo.fit(X['SEXO'])\n",
    "X[\"SEXO\"] = le_sexo.transform(X[\"SEXO\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocesamiento DM (Diabetes Mellitus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_DM = preprocessing.LabelEncoder()\n",
    "le_DM.fit(X['DM'])\n",
    "X[\"DM\"] = le_DM.transform(X[\"DM\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocesamiento LCFA (Limintación crónica del flujo aéreo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_LCFA = preprocessing.LabelEncoder()\n",
    "le_LCFA.fit(X[\"LCFA\"])\n",
    "X[\"LCFA\"] = le_LCFA.transform(X[\"LCFA\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proprocesamiento HTA (Hipertención Arterial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_HTA = preprocessing.LabelEncoder()\n",
    "le_HTA.fit(X[\"HTA\"])\n",
    "X[\"HTA\"] = le_HTA.transform(X[\"HTA\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asignación de la variable dependiente a predecir (categoría), al vector y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h1 id=\"Análisis de componentes principales\">Análisis de componentes principales</h1>\n",
    "<p>\n",
    "Se realizó el análisis de componentes principales con el fin de reducir la dimensionalidad de la base de datos utilizada para la predicción de categoría. El objetivo de reducir la dimensionalidad de la base de datos es agilizar los procesos de entrenamiento y predicción de la categoria de pacientes, además de identificar las variables que presentan una mayor utilidad para esta, descartando las que no aportan indormación a la predicción.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizando el set de datos\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "sc = StandardScaler()\n",
    "X = preprocessing.StandardScaler().fit(X).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos la matriz de covarianza\n",
    "\n",
    "print('NumPy covariance matrix: \\n%s' %np.cov(X.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculamos los autovalores y autovectores de la matriz y los mostramos\n",
    "\n",
    "cov_mat = np.cov(X.T)\n",
    "\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
    "\n",
    "print('Eigenvectors \\n%s' %eig_vecs)\n",
    "print('\\nEigenvalues \\n%s' %eig_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Hacemos una lista de parejas (autovector, autovalor) \n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "\n",
    "# Ordenamos estas parejas den orden descendiente con la función sort\n",
    "eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# Visualizamos la lista de autovalores en orden desdenciente\n",
    "print('Autovalores en orden descendiente:')\n",
    "for i in eig_pairs:\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A partir de los autovalores, calculamos la varianza explicada\n",
    "tot = sum(eig_vals)\n",
    "var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representamos en un diagrama de barras la varianza explicada por cada autovalor, y la acumulada\n",
    "with plt.style.context('seaborn-pastel'):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    plt.bar(range(300), var_exp, alpha=0.5, align='center',\n",
    "            label='Varianza individual explicada', color='g')\n",
    "    plt.step(range(300), cum_var_exp, where='mid', linestyle='--', label='Varianza explicada acumulada')\n",
    "    plt.ylabel('Ratio de Varianza Explicada')\n",
    "    plt.xlabel('Componentes Principales')\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Generamos la matríz a partir de los pares autovalor-autovector\n",
    "matrix_w = np.hstack((eig_pairs[0][1].reshape(300,1),\n",
    "                      eig_pairs[1][1].reshape(300,1)))\n",
    "\n",
    "print('Matriz W:\\n', matrix_w)\n",
    "\n",
    "Y = X.dot(matrix_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for lab, col in zip(('Iris-setosa', 'Iris-versicolor', 'Iris-virginica'),\n",
    "                        ('magenta', 'cyan', 'limegreen')):\n",
    "        plt.scatter(Y[y==lab, 0],\n",
    "                    Y[y==lab, 1],\n",
    "                    label=lab,\n",
    "                    c=col)\n",
    "    plt.xlabel('Componente Principal 1')\n",
    "    plt.ylabel('Componente Principal 2')\n",
    "    plt.legend(loc='lower center')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "#Make an instance of the Model\n",
    "pca = PCA(0.95)\n",
    "pca.fit(X)\n",
    "pca.explained_variance_ratio_\n",
    "pca.n_components_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0:5]\n",
    "A=pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DF_PCA_Desc_evento=pd.merge(df_ID, A, on='ID_PACIENTE')\n",
    "DF_PCA_Desc_evento=pd.concat([df_ID,A], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_PCA_Desc_evento.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocessing.StandardScaler().fit(X).transform(X)\n",
    "X[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_PCA_Desc_evento.to_csv('DF_PCA_Desc_evento.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h1 id=\"Configurando algoritmos\">Configurando algoritmos</h1>\n",
    "<p>\n",
    "En esta sección se definen parámetros necesarios para la correcta aplicación de los algoritmos a implementar, además de seccionar el conjunto de datos en datos de prueba(30%)y de entrenamiento (70%). Los parámetros escogidos pueden ser modificados con el fin de obtener resultados diferentes\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carga de paquetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segmentación del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainset, X_testset, y_trainset, y_testset = train_test_split(X, y, test_size=0.3, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us = NearMiss(sampling_strategy='auto',version=1,n_neighbors=2, n_neighbors_ver3=3,n_jobs=None,)\n",
    "X_trainset_res_1, y_trainset_res_1 = us.fit_sample(X_trainset, y_trainset)\n",
    " \n",
    "print (\"Distribution before resampling {}\".format(Counter(y_trainset)))\n",
    "print (\"Distribution after resampling {}\".format(Counter(y_trainset_res_1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us = NearMiss(sampling_strategy='majority',version=1,n_neighbors=2, n_neighbors_ver3=3,n_jobs=None,)\n",
    "X_trainset_res_2, y_trainset_res_2 = us.fit_sample(X_trainset, y_trainset)\n",
    " \n",
    "print (\"Distribution before resampling {}\".format(Counter(y_trainset)))\n",
    "print (\"Distribution after resampling {}\".format(Counter(y_trainset_res_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us = NearMiss(sampling_strategy='not minority',version=1,n_neighbors=2, n_neighbors_ver3=3,n_jobs=None,)\n",
    "X_trainset_res_3, y_trainset_res_3 = us.fit_sample(X_trainset, y_trainset)\n",
    " \n",
    "print (\"Distribution before resampling {}\".format(Counter(y_trainset)))\n",
    "print (\"Distribution after resampling {}\".format(Counter(y_trainset_res_3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us = NearMiss(sampling_strategy='not majority',version=1,n_neighbors=2, n_neighbors_ver3=3,n_jobs=None,)\n",
    "X_trainset_res_4, y_trainset_res_4 = us.fit_sample(X_trainset, y_trainset)\n",
    " \n",
    "print (\"Distribution before resampling {}\".format(Counter(y_trainset)))\n",
    "print (\"Distribution after resampling {}\".format(Counter(y_trainset_res_4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us = NearMiss(sampling_strategy='all',version=1,n_neighbors=2, n_neighbors_ver3=3,n_jobs=None,)\n",
    "X_trainset_res_5, y_trainset_res_5 = us.fit_sample(X_trainset, y_trainset)\n",
    " \n",
    "print (\"Distribution before resampling {}\".format(Counter(y_trainset)))\n",
    "print (\"Distribution after resampling {}\".format(Counter(y_trainset_res_5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os =  RandomOverSampler(sampling_strategy='auto', random_state=None)\n",
    "X_trainset_res_A, y_trainset_res_A = os.fit_sample(X_trainset, y_trainset)\n",
    " \n",
    "print (\"Distribution before resampling {}\".format(Counter(y_trainset)))\n",
    "print (\"Distribution labels after resampling {}\".format(Counter(y_trainset_res_A)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os =  RandomOverSampler(sampling_strategy='not minority', random_state=None)\n",
    "X_trainset_res_C, y_trainset_res_C = os.fit_sample(X_trainset, y_trainset)\n",
    " \n",
    "print (\"Distribution before resampling {}\".format(Counter(y_trainset)))\n",
    "print (\"Distribution labels after resampling {}\".format(Counter(y_trainset_res_C)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os =  RandomOverSampler(sampling_strategy='not majority', random_state=None)\n",
    "X_trainset_res_D, y_trainset_res_D = os.fit_sample(X_trainset, y_trainset)\n",
    " \n",
    "print (\"Distribution before resampling {}\".format(Counter(y_trainset)))\n",
    "print (\"Distribution labels after resampling {}\".format(Counter(y_trainset_res_D)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os =  RandomOverSampler(sampling_strategy='all', random_state=None)\n",
    "X_trainset_res_E, y_trainset_res_E = os.fit_sample(X_trainset, y_trainset)\n",
    " \n",
    "print (\"Distribution before resampling {}\".format(Counter(y_trainset)))\n",
    "print (\"Distribution labels after resampling {}\".format(Counter(y_trainset_res_E)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE-Tomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_us = SMOTETomek(sampling_strategy='auto',random_state=None,smote=None,tomek=None,n_jobs=None,)\n",
    "X_trainset_res_ST1, y_trainset_res_ST1 = os.fit_sample(X_trainset, y_trainset)\n",
    " \n",
    "print (\"Distribution before resampling {}\".format(Counter(y_trainset)))\n",
    "print (\"Distribution labels after resampling {}\".format(Counter(y_trainset_res_ST1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_us = SMOTETomek(sampling_strategy='majority',random_state=None,smote=None,tomek=None,n_jobs=None,)\n",
    "X_trainset_res_ST2, y_trainset_res_ST2 = os.fit_sample(X_trainset, y_trainset)\n",
    " \n",
    "print (\"Distribution before resampling {}\".format(Counter(y_trainset)))\n",
    "print (\"Distribution labels after resampling {}\".format(Counter(y_trainset_res_ST2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_us = SMOTETomek(sampling_strategy='not minority',random_state=None,smote=None,tomek=None,n_jobs=None,)\n",
    "X_trainset_res_ST3, y_trainset_res_ST3 = os.fit_sample(X_trainset, y_trainset)\n",
    " \n",
    "print (\"Distribution before resampling {}\".format(Counter(y_trainset)))\n",
    "print (\"Distribution labels after resampling {}\".format(Counter(y_trainset_res_ST3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_us = SMOTETomek(sampling_strategy='not majority',random_state=None,smote=None,tomek=None,n_jobs=None,)\n",
    "X_trainset_res_ST4, y_trainset_res_ST4 = os.fit_sample(X_trainset, y_trainset)\n",
    " \n",
    "print (\"Distribution before resampling {}\".format(Counter(y_trainset)))\n",
    "print (\"Distribution labels after resampling {}\".format(Counter(y_trainset_res_ST4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_us = SMOTETomek(sampling_strategy='all',random_state=None,smote=None,tomek=None,n_jobs=None,)\n",
    "X_trainset_res_ST5, y_trainset_res_ST5 = os.fit_sample(X_trainset, y_trainset)\n",
    " \n",
    "print (\"Distribution before resampling {}\".format(Counter(y_trainset)))\n",
    "print (\"Distribution labels after resampling {}\".format(Counter(y_trainset_res_ST5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Árbol de desición"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT= DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4)\n",
    "#pipe = Pipeline(steps=[('pca', pca), ('DT', DT)])\n",
    "DT.fit(X_trainset,y_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT= DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4)\n",
    "DT.fit(X_trainset,y_trainset)\n",
    "DT_Ss1=DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4).fit(X_trainset_res_1,y_trainset_res_1)\n",
    "DT_Ss2=DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4).fit(X_trainset_res_2,y_trainset_res_2)\n",
    "DT_Ss3=DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4).fit(X_trainset_res_3,y_trainset_res_3)\n",
    "DT_Ss4=DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4).fit(X_trainset_res_4,y_trainset_res_4)\n",
    "DT_Ss5=DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4).fit(X_trainset_res_5,y_trainset_res_5)\n",
    "DT_OsA=DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4).fit(X_trainset_res_A,y_trainset_res_A)\n",
    "DT_OsC=DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4).fit(X_trainset_res_C,y_trainset_res_C)\n",
    "DT_OsD=DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4).fit(X_trainset_res_D,y_trainset_res_D)\n",
    "DT_OsE=DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4).fit(X_trainset_res_E,y_trainset_res_E)\n",
    "DT_ST1=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
    "                       max_depth=13, max_features=None, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=1, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0,\n",
    "                       random_state=42, splitter='best').fit(X_trainset_res_ST1,y_trainset_res_ST1)\n",
    "DT_ST2=DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4).fit(X_trainset_res_ST2,y_trainset_res_ST2)\n",
    "DT_ST3=DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4).fit(X_trainset_res_ST3,y_trainset_res_ST3)\n",
    "DT_ST4=DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4).fit(X_trainset_res_ST4,y_trainset_res_ST4)\n",
    "DT_ST5=DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4).fit(X_trainset_res_ST5,y_trainset_res_ST5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Análisis de componentes principales para identificar las variables que originan una mejor clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(X)\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(nrows=2, sharex=True, figsize=(6, 6))\n",
    "ax0.plot(np.arange(1, pca.n_components_ + 1),\n",
    "         pca.explained_variance_ratio_, '+', linewidth=2)\n",
    "ax0.set_ylabel('PCA explained variance ratio')\n",
    "\n",
    "ax0.axvline(search.best_estimator_.named_steps['pca'].n_components,\n",
    "            linestyle=':', label='n_components chosen')\n",
    "ax0.legend(prop=dict(size=12))\n",
    "\n",
    "# For each number of components, find the best classifier results\n",
    "results = pd.DataFrame(search.cv_results_)\n",
    "components_col = 'param_pca__n_components'\n",
    "best_clfs = results.groupby(components_col).apply(\n",
    "    lambda g: g.nlargest(1, 'mean_test_score'))\n",
    "\n",
    "best_clfs.plot(x=components_col, y='mean_test_score', yerr='std_test_score',\n",
    "               legend=False, ax=ax1)\n",
    "ax1.set_ylabel('Classification accuracy (val)')\n",
    "ax1.set_xlabel('n_components')\n",
    "\n",
    "plt.xlim(-1, 70)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_1 = DT.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_1_1 =DT_Ss1.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_1_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_1_2 = DT_Ss2.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_1_2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_1_3 = DT_Ss3.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_1_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_1_4 = DT_Ss4.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_1_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_1_5 = DT_Ss5.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_1_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_1_A = DT_OsA.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_1_A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_1_C = DT_OsC.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_1_C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_1_D = DT_OsD.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_1_D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_1_E = DT_OsE.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_1_E))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_1_ST1 = DT_ST1.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_1_ST1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_1_ST2 = DT_ST2.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_1_ST2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_1_ST3 = DT_ST3.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_1_ST3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_1_ST4 = DT.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_1_ST4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_1_ST5 = DT_ST5.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_1_ST5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_prob_1=DT.predict_proba(X_testset)\n",
    "DT_Acc=round(metrics.accuracy_score(y_testset,  yhat_1),4)\n",
    "DT_F1=f1_score(y_testset, yhat_1, average='weighted') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mejores parámetros obtenidos a travez de grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_GS =DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
    "                       max_depth=13, max_features=None, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=1, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0,\n",
    "                       random_state=42, splitter='best')\n",
    "DT_GS.fit(X_trainset,y_trainset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicación de grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_parameters = [{'criterion': ['entropy', 'gini'], 'max_depth': np.arange(3, 21)},{'min_samples_leaf': [2,5, 10, 20, 50, 100]}]\n",
    "DT_GS = GridSearchCV(DecisionTreeClassifier(random_state=42), DT_parameters, verbose=1, cv=5, scoring='balanced_accuracy')\n",
    "DT_GS.fit(X_trainset,y_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_GS.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_1_GS= DT_GS.predict(X_testset)\n",
    "DT_Acc_GS=round(metrics.accuracy_score(y_testset, yhat_1_GS),4)\n",
    "DT_Acc_ST1=round(metrics.accuracy_score(y_testset, yhat_1_ST1),4)\n",
    "DT_F1_GS=round(f1_score(y_testset, yhat_1_GS, average='weighted'),4) \n",
    "DT_F1_ST1=round(f1_score(y_testset, yhat_1_ST1, average='weighted'),4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados_DT = {'índices de rendimiento':['Accuracy','F1-Score'],\n",
    "             'Árboles de decisión':[DT_Acc,DT_F1],\n",
    "             'Grid Search':[DT_Acc_GS,DT_F1_GS],\n",
    "             'Smote-Tomek':[DT_Acc_ST1,DT_F1_ST1]}\n",
    "Tabla_resultados_DT=pd.DataFrame(resultados_DT)\n",
    "print(Tabla_resultados_DT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instalación de paquete eli5 para aplicar técnica de \"Permutation Importance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = PermutationImportance(DT, random_state=1).fit(X, y)\n",
    "eli5.show_weights(perm, feature_names =X.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = PermutationImportance(DT_GS, random_state=1).fit(X, y)\n",
    "eli5.show_weights(perm, feature_names =X.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances = DT.feature_importances_\n",
    "indices = np.argsort(feat_importances)\n",
    "features = X.columns\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title('Features +importante')\n",
    "plt.barh(range(len(indices)), feat_importances[indices], color='g', align='center',linestyle=\"solid\",alpha=0.8)\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Importancia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importación de paquetes para aplicar SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "ex = shap.TreeExplainer(DT)\n",
    "shap_values = ex.shap_values(X)\n",
    "shap.summary_plot(shap_values, X,max_display=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -c conda-forge pdpbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from pdpbox import pdp, get_dataset, info_plots\n",
    "\n",
    "# Create the data that we will plot\n",
    "pdp_goals = pdp.pdp_isolate(model=DT_GS, dataset=X, model_features=X.columns, feature='EVA')\n",
    "\n",
    "# plot it\n",
    "pdp.pdp_plot(pdp_goals, 'EVA')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kerner Explainer\n",
    "explainer = shap.KernelExplainer(DT.predict_proba,X[:100])\n",
    "shap_values = explainer.shap_values(X[:100])\n",
    "shap.summary_plot(shap_values, X[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[0], X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "columIndex= 2\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1][columIndex,:], X.iloc[columIndex,:], link=\"logit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bosque de desición"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RF= RandomForestClassifier(max_depth=2, random_state=0)\n",
    "RF.fit(X_trainset,y_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_Ss1= RandomForestClassifier(max_depth=2, random_state=0).fit(X_trainset_res_1,y_trainset_res_1)\n",
    "RF_Ss2= RandomForestClassifier(max_depth=2, random_state=0).fit(X_trainset_res_2,y_trainset_res_2)\n",
    "RF_Ss3= RandomForestClassifier(max_depth=2, random_state=0).fit(X_trainset_res_3,y_trainset_res_3)\n",
    "RF_Ss4= RandomForestClassifier(max_depth=2, random_state=0).fit(X_trainset_res_4,y_trainset_res_4)\n",
    "RF_Ss5= RandomForestClassifier(max_depth=2, random_state=0).fit(X_trainset_res_5,y_trainset_res_5)\n",
    "RF_OsA= RandomForestClassifier(max_depth=2, random_state=0).fit(X_trainset_res_A,y_trainset_res_A)\n",
    "RF_OsC= RandomForestClassifier(max_depth=2, random_state=0).fit(X_trainset_res_C,y_trainset_res_C)\n",
    "RF_OsD= RandomForestClassifier(max_depth=2, random_state=0).fit(X_trainset_res_D,y_trainset_res_D)\n",
    "RF_OsE= RandomForestClassifier(max_depth=2, random_state=0).fit(X_trainset_res_E,y_trainset_res_E)\n",
    "RF_ST1= RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
    "                       criterion='gini', max_depth=13, max_features='auto',\n",
    "                       max_leaf_nodes=None, max_samples=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=1, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=300,\n",
    "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
    "                       warm_start=False).fit(X_trainset_res_ST1,y_trainset_res_ST1)\n",
    "RF_ST2= RandomForestClassifier(max_depth=2, random_state=0).fit(X_trainset_res_ST2,y_trainset_res_ST2)\n",
    "RF_ST3= RandomForestClassifier(max_depth=2, random_state=0).fit(X_trainset_res_ST3,y_trainset_res_ST3)\n",
    "RF_ST4= RandomForestClassifier(max_depth=2, random_state=0).fit(X_trainset_res_ST4,y_trainset_res_ST4)\n",
    "RF_ST5= RandomForestClassifier(max_depth=2, random_state=0).fit(X_trainset_res_ST5,y_trainset_res_ST5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_2 = RF.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_2_1 = RF_Ss1.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_2_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_2_2 = RF_Ss2.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_2_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_2_3 = RF_Ss3.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_2_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_2_4 = RF_Ss4.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_2_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_2_5 = RF_Ss5.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_2_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_2_A = RF_OsA.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_2_A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_2_C = RF_OsC.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_2_C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_2_D = RF_OsD.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_2_D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_2_E = RF_OsE.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_2_E))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_2_ST1 = RF_ST1.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_2_ST1 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_2_ST2 = RF_ST2.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_2_ST2 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_2_ST3 = RF_ST3.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_2_ST3 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_2_ST4 = RF_ST4.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_2_ST4 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_2_ST5 = RF_ST5.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_2_ST5 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_prob_2=RF.predict_proba(X_testset)\n",
    "RF_Acc=round(metrics.accuracy_score(y_testset, yhat_2),4)\n",
    "#RF_Jcc=round(jaccard_similarity_score(y_testset, yhat_2),4)\n",
    "RF_F1=f1_score(y_testset, yhat_2, average='weighted') \n",
    "RF_lgl=round(log_loss(y_testset, yhat_prob_2),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_GS=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
    "                       criterion='gini', max_depth=13, max_features='auto',\n",
    "                       max_leaf_nodes=None, max_samples=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=1, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=300,\n",
    "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
    "                       warm_start=False)\n",
    "RF_GS.fit(X_trainset,y_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_parameters ={ 'n_estimators': [100,200,300,400,500],'criterion': ['entropy', 'gini'], 'max_depth': np.arange(3, 21)},{'min_samples_leaf': [2,5, 10, 20, 50, 100]}\n",
    "RF_GS = GridSearchCV(estimator=RF,param_grid=RF_parameters, cv= 5,scoring=\"accuracy\")\n",
    "RF_GS.fit(X_trainset,y_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_GS.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_2_GS= RF_GS.predict(X_testset)\n",
    "yhat_prob_2_GS=RF_GS.predict_proba(X_testset)\n",
    "RF_Acc_GS=round(metrics.accuracy_score(y_testset, yhat_2_GS),4)\n",
    "RF_Acc_ST1=round(metrics.accuracy_score(y_testset, yhat_2_ST1),4)\n",
    "#RF_Jcc_GS=round(jaccard_similarity_score(y_testset, yhat_2_GS),4)\n",
    "#RF_lgl_GS=round(log_loss(y_testset, yhat_prob_2_GS),4)\n",
    "RF_F1_GS=round(f1_score(y_testset, yhat_2_GS, average='weighted'),4) \n",
    "RF_F1_ST1=round(f1_score(y_testset, yhat_2_ST1, average='weighted'),4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resultados_RF = {'índices de rendimiento':['Accuracy','F1-Score'],\n",
    "             'Bosque de decisión':[RF_Acc, RF_F1],\n",
    "             'Grid Search':[RF_Acc_GS,RF_F1_GS],\n",
    "                'Smote Tomek':[RF_Acc_ST1,RF_F1_ST1]}\n",
    "Tabla_resultados_RF=pd.DataFrame(resultados_RF)\n",
    "print(Tabla_resultados_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = PermutationImportance(RF_GS, random_state=1).fit(X, y)\n",
    "eli5.show_weights(perm, feature_names =X.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import shap\n",
    "ex = shap.TreeExplainer(RF_GS)\n",
    "shap_values = ex.shap_values(X_testset)\n",
    "shap.summary_plot(shap_values, X_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kerner Explainer\n",
    "explainer = shap.KernelExplainer(RF.predict_proba,X[:100])\n",
    "shap_values = explainer.shap_values(X[:100])\n",
    "shap.summary_plot(shap_values, X[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regresión logística (multi-class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "LR = LogisticRegression(multi_class='ovr',class_weight='balanced', max_iter=1000,penalty='l2',C=1).fit(X_trainset,y_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "LR = LogisticRegression(multi_class='ovr',class_weight='balanced', max_iter=1000,penalty='l2',C=1)\n",
    "pca = PCA()\n",
    "pipe = Pipeline(steps=[('pca', pca), ('LR', LR)])\n",
    "param_grid = {\n",
    "    'pca__n_components': [2,5, 15, 30, 45, 64,100,300],\n",
    "    \n",
    "}\n",
    "search = GridSearchCV(pipe, param_grid, n_jobs=-1)\n",
    "search.fit(X, y)\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(X)\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(nrows=2, sharex=True, figsize=(6, 6))\n",
    "ax0.plot(np.arange(1, pca.n_components_ + 1),\n",
    "         pca.explained_variance_ratio_, '+', linewidth=2)\n",
    "ax0.set_ylabel('PCA explained variance ratio')\n",
    "\n",
    "ax0.axvline(search.best_estimator_.named_steps['pca'].n_components,\n",
    "            linestyle=':', label='n_components chosen')\n",
    "ax0.legend(prop=dict(size=12))\n",
    "\n",
    "# For each number of components, find the best classifier results\n",
    "results = pd.DataFrame(search.cv_results_)\n",
    "components_col = 'param_pca__n_components'\n",
    "best_clfs = results.groupby(components_col).apply(\n",
    "    lambda g: g.nlargest(1, 'mean_test_score'))\n",
    "\n",
    "best_clfs.plot(x=components_col, y='mean_test_score', yerr='std_test_score',\n",
    "               legend=False, ax=ax1)\n",
    "ax1.set_ylabel('Classification accuracy (val)')\n",
    "ax1.set_xlabel('n_components')\n",
    "\n",
    "plt.xlim(-1, 70)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "LR = LogisticRegression(multi_class='ovr',class_weight='balanced', max_iter=1000,penalty='l2',C=1).fit(X_trainset,y_trainset)\n",
    "LR_Ss1 = LogisticRegression(multi_class='ovr',class_weight='balanced', max_iter=1000).fit(X_trainset_res_1,y_trainset_res_1)\n",
    "LR_Ss2 = LogisticRegression(multi_class='ovr',class_weight='balanced', max_iter=1000).fit(X_trainset_res_2,y_trainset_res_2)\n",
    "LR_Ss3 = LogisticRegression(multi_class='ovr',class_weight='balanced', max_iter=1000).fit(X_trainset_res_3,y_trainset_res_3)\n",
    "LR_Ss4 = LogisticRegression(multi_class='ovr',class_weight='balanced', max_iter=1000).fit(X_trainset_res_4,y_trainset_res_4)\n",
    "LR_Ss5 = LogisticRegression(multi_class='ovr',class_weight='balanced', max_iter=1000).fit(X_trainset_res_5,y_trainset_res_5)\n",
    "LR_OsA = LogisticRegression(multi_class='ovr',class_weight='balanced', max_iter=1000).fit(X_trainset_res_A,y_trainset_res_A)\n",
    "LR_OsC = LogisticRegression(multi_class='ovr',class_weight='balanced', max_iter=1000).fit(X_trainset_res_C,y_trainset_res_C)\n",
    "LR_OsD = LogisticRegression(multi_class='ovr',class_weight='balanced', max_iter=1000).fit(X_trainset_res_D,y_trainset_res_D)\n",
    "LR_OsE = LogisticRegression(multi_class='ovr',class_weight='balanced', max_iter=1000).fit(X_trainset_res_E,y_trainset_res_E)\n",
    "LR_ST1 = LogisticRegression(C=0.1, class_weight='balanced', dual=False,\n",
    "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
    "                   max_iter=1000, multi_class='ovr', n_jobs=None, penalty='l2',\n",
    "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
    "                   warm_start=False).fit(X_trainset_res_ST1,y_trainset_res_ST1)\n",
    "LR_ST2 = LogisticRegression(multi_class='ovr',class_weight='balanced', max_iter=1000).fit(X_trainset_res_ST2,y_trainset_res_ST2)\n",
    "LR_ST3 = LogisticRegression(multi_class='ovr',class_weight='balanced', max_iter=1000).fit(X_trainset_res_ST3,y_trainset_res_ST3)\n",
    "LR_ST4 = LogisticRegression(multi_class='ovr',class_weight='balanced', max_iter=1000).fit(X_trainset_res_ST4,y_trainset_res_ST4)\n",
    "LR_ST5 = LogisticRegression(multi_class='ovr',class_weight='balanced', max_iter=1000).fit(X_trainset_res_ST5,y_trainset_res_ST5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_3= LR.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_3_1= LR_Ss1.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_3_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_3_2= LR_Ss2.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_3_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_3_3= LR_Ss3.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_3_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_3_4= LR_Ss4.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_3_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_3_5= LR_Ss5.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_3_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_3_A= LR_OsA.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_3_A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_3_C= LR_OsC.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_3_C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_3_D= LR_OsD.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_3_D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_3_E= LR_OsE.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_3_E))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_3_ST1= LR_ST1.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_3_ST1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_3_ST2= LR_ST2.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_3_ST2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_3_ST3= LR_ST3.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_3_ST3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_3_ST4= LR_ST4.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_3_ST4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_3_ST5= LR_ST5.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_3_ST5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_prob_3_4=LR_Ss4.predict_proba(X_testset)\n",
    "LR_Acc=round(metrics.accuracy_score(y_testset, yhat_3_4),4)\n",
    "#LR_Jcc=round(jaccard_similarity_score(y_testset, yhat_3_4),4)\n",
    "#LR_lgl=round(log_loss(y_testset, yhat_3_4),4)\n",
    "LR_F1=f1_score(y_testset, yhat_3_4, average='weighted') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_GS =LogisticRegression(C=0.1, class_weight='balanced', dual=False,\n",
    "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
    "                   max_iter=1000, multi_class='ovr', n_jobs=None, penalty='l2',\n",
    "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
    "                   warm_start=False)\n",
    "LR_GS.fit(X_trainset,y_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LR_parameters ={'C':(0.1,1,10), 'penalty':['l1','l2','elasticnet']}\n",
    "LR_GS = GridSearchCV(estimator=LR_Ss4,param_grid=LR_parameters, cv= 5,verbose=0,)\n",
    "LR_GS.fit(X_trainset,y_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_GS.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_3_GS= LR_GS.predict(X_testset)\n",
    "yhat_prob_3_GS=LR_GS.predict_proba(X_testset)\n",
    "LR_Acc_GS=round(metrics.accuracy_score(y_testset, yhat_3_GS),4)\n",
    "LR_Acc_ST1=round(metrics.accuracy_score(y_testset, yhat_3_ST1),4)\n",
    "#LR_Jcc_GS=round(jaccard_similarity_score(y_testset, yhat_3_GS),4)\n",
    "#LR_lgl_GS=round(log_loss(y_testset, yhat_prob_3_GS),4)\n",
    "LR_F1_GS=round(f1_score(y_testset, yhat_3_GS, average='weighted'),4) \n",
    "LR_F1_ST1=round(f1_score(y_testset, yhat_3_ST1, average='weighted'),4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados_LR = {'índices de rendimiento':['Accuracy','F1-Score'],\n",
    "             'Regresión Logística':[LR_Acc,LR_F1],\n",
    "             'Grid Search':[LR_Acc_GS,LR_F1_GS],\n",
    "                'Smote Tomek':[LR_Acc_ST1,LR_F1_ST1]}\n",
    "Tabla_resultados_LR=pd.DataFrame(resultados_LR)\n",
    "print(Tabla_resultados_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = PermutationImportance(LR_GS, random_state=1).fit(X, y)\n",
    "eli5.show_weights(perm, feature_names =X.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kerner Explainer\n",
    "explainer = shap.KernelExplainer(LR.predict_proba,X[:100])\n",
    "shap_values = explainer.shap_values(X[:100])\n",
    "shap.summary_plot(shap_values, X[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redes Neuronales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "NN = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1,max_iter=500).fit(X_trainset,y_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "NN = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "pca = PCA()\n",
    "pipe = Pipeline(steps=[('pca', pca), ('NN', NN)])\n",
    "param_grid = {\n",
    "    'pca__n_components': [2,5, 15, 30, 45, 64,100,300],\n",
    "    \n",
    "}\n",
    "search = GridSearchCV(pipe, param_grid, n_jobs=-1)\n",
    "search.fit(X, y)\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(X)\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(nrows=2, sharex=True, figsize=(6, 6))\n",
    "ax0.plot(np.arange(1, pca.n_components_ + 1),\n",
    "         pca.explained_variance_ratio_, '+', linewidth=2)\n",
    "ax0.set_ylabel('PCA explained variance ratio')\n",
    "\n",
    "ax0.axvline(search.best_estimator_.named_steps['pca'].n_components,\n",
    "            linestyle=':', label='n_components chosen')\n",
    "ax0.legend(prop=dict(size=12))\n",
    "\n",
    "# For each number of components, find the best classifier results\n",
    "results = pd.DataFrame(search.cv_results_)\n",
    "components_col = 'param_pca__n_components'\n",
    "best_clfs = results.groupby(components_col).apply(\n",
    "    lambda g: g.nlargest(1, 'mean_test_score'))\n",
    "\n",
    "best_clfs.plot(x=components_col, y='mean_test_score', yerr='std_test_score',\n",
    "               legend=False, ax=ax1)\n",
    "ax1.set_ylabel('Classification accuracy (val)')\n",
    "ax1.set_xlabel('n_components')\n",
    "\n",
    "plt.xlim(-1, 70)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_Ss1 = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1,max_iter=1000).fit(X_trainset_res_1,y_trainset_res_1)\n",
    "NN_Ss2 = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1,max_iter=1000).fit(X_trainset_res_2,y_trainset_res_2)\n",
    "NN_Ss3 = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1,max_iter=1000).fit(X_trainset_res_3,y_trainset_res_3)\n",
    "NN_Ss4 = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1,max_iter=1000).fit(X_trainset_res_4,y_trainset_res_4)\n",
    "NN_Ss5 = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1,max_iter=1000).fit(X_trainset_res_5,y_trainset_res_5)\n",
    "NN_OsA = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1,max_iter=1000).fit(X_trainset_res_A,y_trainset_res_A)\n",
    "NN_OsC = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1,max_iter=1000).fit(X_trainset_res_C,y_trainset_res_C)\n",
    "NN_OsD = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1,max_iter=1000).fit(X_trainset_res_D,y_trainset_res_D)\n",
    "NN_OsE = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1,max_iter=1000).fit(X_trainset_res_E,y_trainset_res_E)\n",
    "NN_ST1 = MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
    "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
    "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
    "              learning_rate_init=0.001, max_fun=15000, max_iter=1000,\n",
    "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
    "              power_t=0.5, random_state=1, shuffle=True, solver='sgd',\n",
    "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
    "              warm_start=False).fit(X_trainset_res_ST1,y_trainset_res_ST1)\n",
    "NN_ST2 = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1,max_iter=1000).fit(X_trainset_res_ST2,y_trainset_res_ST2)\n",
    "NN_ST3 = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1,max_iter=1000).fit(X_trainset_res_ST3,y_trainset_res_ST3)\n",
    "NN_ST4 = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1,max_iter=1000).fit(X_trainset_res_ST4,y_trainset_res_ST4)\n",
    "NN_ST5 = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1,max_iter=1000).fit(X_trainset_res_ST5,y_trainset_res_ST5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_4 = NN.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_4_1 = NN_Ss1.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_4_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_4_2 = NN_Ss2.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_4_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_4_3 = NN_Ss3.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_4_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_4_4 = NN_Ss4.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_4_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_4_5 = NN_Ss5.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_4_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_4_A = NN_OsA.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_4_A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_4_C = NN_OsC.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_4_C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_4_D = NN_OsD.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_4_D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_4_E = NN_OsE.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_4_E))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_4_ST1 = NN_ST1.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_4_ST1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_4_ST2 = NN_ST2.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_4_ST2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_4_ST3 = NN_ST3.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_4_ST3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_4_ST4 = NN_ST4.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_4_ST4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_4_ST5 = NN_ST5.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_4_ST5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_prob_4=NN.predict_proba(X_testset)\n",
    "NN_Acc=round(metrics.accuracy_score(y_testset, yhat_4),4)\n",
    "#NN_Jcc=round(jaccard_similarity_score(y_testset, yhat_4),4)\n",
    "NN_lgl=round(log_loss(y_testset, yhat_prob_4),4)\n",
    "NN_F1=f1_score(y_testset, yhat_4, average='weighted') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_GS =MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
    "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
    "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
    "              learning_rate_init=0.001, max_fun=15000, max_iter=400,\n",
    "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
    "              power_t=0.5, random_state=1, shuffle=True, solver='sgd',\n",
    "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
    "              warm_start=False)\n",
    "NN_GS.fit(X_trainset,y_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_parameters = {'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],'alpha': [0.0001, 0.05],'learning_rate': ['constant','adaptive'],'max_iter':[300,400]}\n",
    "\n",
    "NN_GS = GridSearchCV(estimator=NN,param_grid=NN_parameters, cv= 5, verbose=True,scoring=\"accuracy\")\n",
    "NN_GS.fit(X_trainset,y_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_GS.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_4_GS= NN_GS.predict(X_testset)\n",
    "yhat_prob_4_GS=NN_GS.predict_proba(X_testset)\n",
    "NN_Acc_GS=round(metrics.accuracy_score(y_testset, yhat_4_GS),4)\n",
    "NN_Acc_ST1=round(metrics.accuracy_score(y_testset, yhat_4_ST1),4)\n",
    "#NN_Jcc_GS=round(jaccard_similarity_score(y_testset, yhat_4_GS),4)\n",
    "#NN_lgl_GS=round(log_loss(y_testset, yhat_prob_4_GS),4)\n",
    "NN_F1_GS=round(f1_score(y_testset, yhat_4_GS, average='weighted'),4) \n",
    "NN_F1_ST1=round(f1_score(y_testset, yhat_4_ST1, average='weighted'),4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados_NN = {'índices de rendimiento':['Accuracy','F1-Score'],\n",
    "             'Red Neuronal':[NN_Acc,NN_F1],\n",
    "             'Grid Search':[NN_Acc_GS,NN_F1_GS],\n",
    "                'Smote-Tomek':[NN_Acc_ST1,NN_F1_ST1]}\n",
    "Tabla_resultados_NN=pd.DataFrame(resultados_NN)\n",
    "print(Tabla_resultados_NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_DF=pd.DataFrame(X, columns=df_cat.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "perm = PermutationImportance(NN_GS, random_state=1).fit(X, y)\n",
    "eli5.show_weights(perm, feature_names =data_DF.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kerner Explainer\n",
    "import shap\n",
    "explainer = shap.KernelExplainer(NN.predict_proba,data_DF[:100])\n",
    "shap_values = explainer.shap_values(data_DF[:100])\n",
    "shap.summary_plot(shap_values, data_DF[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "SVM = svm.SVC(kernel='rbf',decision_function_shape='ovo', probability=True).fit(X_trainset,y_trainset)\n",
    "SVM_Ss1 = svm.SVC(kernel='rbf',decision_function_shape='ovo', probability=True).fit(X_trainset_res_1,y_trainset_res_1)\n",
    "SVM_Ss2 = svm.SVC(kernel='rbf',decision_function_shape='ovo', probability=True).fit(X_trainset_res_2,y_trainset_res_2)\n",
    "SVM_Ss3 = svm.SVC(kernel='rbf',decision_function_shape='ovo', probability=True).fit(X_trainset_res_3,y_trainset_res_3)\n",
    "SVM_Ss4 = svm.SVC(kernel='rbf',decision_function_shape='ovo', probability=True).fit(X_trainset_res_4,y_trainset_res_4)\n",
    "SVM_Ss5 = svm.SVC(kernel='rbf',decision_function_shape='ovo', probability=True).fit(X_trainset_res_5,y_trainset_res_5)\n",
    "SVM_OsA = svm.SVC(kernel='rbf',decision_function_shape='ovo', probability=True).fit(X_trainset_res_A,y_trainset_res_A)\n",
    "SVM_OsC = svm.SVC(kernel='rbf',decision_function_shape='ovo', probability=True).fit(X_trainset_res_C,y_trainset_res_C)\n",
    "SVM_OsD = svm.SVC(kernel='rbf',decision_function_shape='ovo', probability=True).fit(X_trainset_res_D,y_trainset_res_D)\n",
    "SVM_OsE = svm.SVC(kernel='rbf',decision_function_shape='ovo', probability=True).fit(X_trainset_res_E,y_trainset_res_E)\n",
    "SVM_ST1 = svm.SVC(kernel='rbf',decision_function_shape='ovo', probability=True).fit(X_trainset_res_ST1,y_trainset_res_ST1)\n",
    "SVM_ST2 = svm.SVC(kernel='rbf',decision_function_shape='ovo', probability=True).fit(X_trainset_res_ST2,y_trainset_res_ST2)\n",
    "SVM_ST3 = svm.SVC(kernel='rbf',decision_function_shape='ovo', probability=True).fit(X_trainset_res_ST3,y_trainset_res_ST3)\n",
    "SVM_ST4 = svm.SVC(kernel='rbf',decision_function_shape='ovo', probability=True).fit(X_trainset_res_ST4,y_trainset_res_ST4)\n",
    "SVM_ST5 = svm.SVC(kernel='rbf',decision_function_shape='ovo', probability=True).fit(X_trainset_res_ST5,y_trainset_res_ST5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "SVM = svm.SVC(kernel='rbf',decision_function_shape='ovo', probability=True).fit(X_trainset,y_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_Ss1 = svm.SVC(kernel='rbf',decision_function_shape='ovo', probability=True).fit(X_trainset_res_1,y_trainset_res_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_Ss2 = svm.SVC(kernel='rbf',decision_function_shape='ovo', probability=True).fit(X_trainset_res_2,y_trainset_res_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_Ss3 = svm.SVC(kernel='rbf',decision_function_shape='ovo', probability=True).fit(X_trainset_res_3,y_trainset_res_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_Ss4 = svm.SVC(kernel='rbf',decision_function_shape='ovo', probability=True).fit(X_trainset_res_4,y_trainset_res_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_Ss5 = svm.SVC(kernel='rbf',decision_function_shape='ovo', probability=True).fit(X_trainset_res_5,y_trainset_res_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_OsA = svm.SVC(kernel='rbf',decision_function_shape='ovo', probability=True).fit(X_trainset_res_A,y_trainset_res_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_OsC = svm.SVC(kernel='rbf',decision_function_shape='ovo', probability=True).fit(X_trainset_res_C,y_trainset_res_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_OsD = svm.SVC(kernel='rbf',decision_function_shape='ovo', probability=True).fit(X_trainset_res_D,y_trainset_res_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_OsE = svm.SVC(kernel='rbf',decision_function_shape='ovo', probability=True).fit(X_trainset_res_E,y_trainset_res_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_ST1 = svm.SVC(kernel='rbf',decision_function_shape='ovo', probability=True).fit(X_trainset_res_ST1,y_trainset_res_ST1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_ST2 = svm.SVC(kernel='rbf',decision_function_shape='ovo', probability=True).fit(X_trainset_res_ST2,y_trainset_res_ST2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_ST3 = svm.SVC(kernel='rbf',decision_function_shape='ovo', probability=True).fit(X_trainset_res_ST3,y_trainset_res_ST3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_ST4 = svm.SVC(kernel='rbf',decision_function_shape='ovo', probability=True).fit(X_trainset_res_ST4,y_trainset_res_ST4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_5 = SVM.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_5_1 = SVM_Ss1.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_5_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_5_2 = SVM_Ss2.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_5_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_5_3 = SVM_Ss3.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_5_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_5_4 = SVM_Ss4.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_5_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_5_5 = SVM_Ss5.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_5_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_5_A = SVM_OsA.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_5_A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_5_C = SVM_OsC.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_5_C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_5_D = SVM_OsD.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_5_D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_5_E = SVM_OsE.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_5_E))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_5_ST1 = SVM_ST1.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_5_ST1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_5_ST2 = SVM_ST2.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_5_ST2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_5_ST3 = SVM_ST3.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_5_ST3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_5_ST4 = SVM_ST4.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_5_ST4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_5_ST5 = SVM_ST5.predict(X_testset)\n",
    "print (classification_report(y_testset, yhat_5_ST5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_prob_5=SVM.predict_proba(X_testset)\n",
    "SVM_Acc=round(metrics.accuracy_score(y_testset, yhat_5),4)\n",
    "#SVM_Jcc=round(jaccard_similarity_score(y_testset, yhat_5),4)\n",
    "SVM_lgl=round(log_loss(y_testset, yhat_prob_5),4)\n",
    "SVM_F1=f1_score(y_testset, yhat_5, average='weighted') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_GS =svm.SVC(C=100, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
    "    decision_function_shape='ovo', degree=3, gamma=0.001, kernel='rbf',\n",
    "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
    "    verbose=False)\n",
    "SVM_GS.fit(X_trainset,y_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_parameters ={'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'poly', 'sigmoid']}\n",
    "SVM_GS = GridSearchCV(estimator=SVM,param_grid=SVM_parameters, cv= 3, verbose=True, n_jobs=-1)\n",
    "SVM_GS.fit(X_trainset,y_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_GS.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_5_GS= SVM_GS.predict(X_testset)\n",
    "yhat_prob_5_GS=SVM_GS.predict_proba(X_testset)\n",
    "SVM_Acc_GS=round(metrics.accuracy_score(y_testset, yhat_5_GS),4)\n",
    "#SVM_Jcc_GS=round(jaccard_similarity_score(y_testset, yhat_5_GS),4)\n",
    "SVM_lgl_GS=round(log_loss(y_testset, yhat_prob_5_GS),4)\n",
    "SVM_F1_GS=round(f1_score(y_testset, yhat_5_GS, average='weighted'),4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados_SVM = {'índices de rendimiento':['Accuracy','LogLoss','F1-Score'],\n",
    "             'Support Vactor Machine':[SVM_Acc,SVM_lgl,SVM_F1],\n",
    "             'Grid Search':[SVM_Acc_GS,SVM_lgl_GS,SVM_F1_GS]}\n",
    "Tabla_resultados_SVM=pd.DataFrame(resultados_SVM)\n",
    "print(Tabla_resultados_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "ex = shap.TreeExplainer(SVM)\n",
    "shap_values = ex.shap_values(X_testset)\n",
    "shap.summary_plot(shap_values, X_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kerner Explainer\n",
    "explainer = shap.KernelExplainer(SVM.predict_proba,X[:100])\n",
    "shap_values = explainer.shap_values(X[:100])\n",
    "shap.summary_plot(shap_values, X[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados = {'Algoritmos de clasificación':['Árboles de decisión','AD+GS','Bosques de decisión','BD+GS','Regresión Logística','RL+GS','Red Neuronal','NN+GS'],#'Support Vector Machine','SVM+GS'],\n",
    "             'Accuracy':[DT_Acc,DT_Acc_GS,RF_Acc,RF_Acc_GS,LR_Acc,LR_Acc_GS,NN_Acc,NN_Acc_GS],#SVM_Acc,SVM_Acc_GS],\n",
    "             #'Jaccard':[DT_Jcc,DT_Jcc_GS,RF_Jcc,RF_Jcc_GS,LR_Jcc,LR_Jcc_GS,NN_Jcc,NN_Jcc_GS],#SVM_Jcc,SVM_Jcc_GS],\n",
    "             #'LogLoss':[DT_lgl,DT_lgl_GS,RF_lgl,RF_lgl_GS,LR_lgl,LR_lgl_GS,NN_lgl,NN_lgl_GS],#SVM_lgl,SVM_lgl_GS],\n",
    "             'F1-Score':[DT_F1,DT_F1_GS,RF_F1,RF_F1_GS,LR_F1,LR_F1_GS,NN_F1,NN_F1_GS]}#,SVM_F1,SVM_F1_GS]}\n",
    "Tabla_resultados=pd.DataFrame(resultados)\n",
    "print(Tabla_resultados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h1 id=\"Matrices de confusión\">Matrices de confusión</h1>\n",
    "<p>\n",
    "En esta sección se construyen matrices de confusión para evaluar la eficiaca de cada algoritmo\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Matriz de confusión',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    Esta función muestra y dibuja la matriz de confusión.\n",
    "    La normalización se puede aplicar estableciendo el valor `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Matriz de confusión normalizada\")\n",
    "    else:\n",
    "        print('Matriz de confusión sin normalización')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Etiqueta Real')\n",
    "    plt.xlabel('Etiqueta Predicha')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Árbol de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix1 = confusion_matrix(y_testset, yhat_1_ST1, labels=['C1','C2','C3','C4'])\n",
    "#cnf_matrix2 = confusion_matrix(y_testset, yhat_1_GS, labels=['C1','C2','C3','C4'])\n",
    "np.set_printoptions(precision=2)\n",
    "fig, axs = plt.subplots(1, 2, figsize=(9, 3), sharey=True)\n",
    "axs[0]=plot_confusion_matrix(cnf_matrix1, classes=['C1','C2','C3','C4'],normalize= False,  title='Matriz de confusión árbol de decisión + ST')\n",
    "#axs[1]=plot_confusion_matrix(cnf_matrix2, classes=['C1','C2','C3','C4'],normalize= False,  title='Matriz de confusión árbol de decisión')\n",
    "print (classification_report(y_testset, yhat_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bosque de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(y_testset, yhat_2_GS, labels=['C1','C2','C3','C4'])\n",
    "np.set_printoptions(precision=2)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['C1','C2','C3','C4'],normalize= False,  title='Matriz de confusión bosque de decisión +GS')\n",
    "print (classification_report(y_testset,yhat_1_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regresión Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(y_testset, yhat_3_GS, labels=['C1','C2','C3','C4'])\n",
    "np.set_printoptions(precision=2)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['C1','C2','C3','C4'],normalize= False,  title='Matriz de confusión regresión logística +GS')\n",
    "print (classification_report(y_testset, yhat_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(y_testset, yhat_4_ST1, labels=['C1','C2','C3','C4'])\n",
    "np.set_printoptions(precision=2)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['C1','C2','C3','C4'],normalize= False,  title='Matriz de confusión red neuronal +ST')\n",
    "print (classification_report(y_testset, yhat_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(y_testset, yhat_5, labels=['C1','C2','C3','C4'])\n",
    "np.set_printoptions(precision=2)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['C1','C2','C3','C4'],normalize= False,  title='Matriz de confusión SVM')\n",
    "print (classification_report(y_testset, yhat_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
